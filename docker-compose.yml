services:
  searxng:
    image: searxng/searxng
    ports:
      - "8080:8080"
    volumes:
      - ./config:/etc/searxng
    environment:
      - SEARXNG_BASE_URL=http://localhost:8080/
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 500M

  qwen_langchain:
    build:
      context: ./qwen_langchain
    ports:
      - "8081:8081"
    volumes:
      - ./qwen_langchain:/app
    environment:
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    extra_hosts:
      - "host.docker.internal:host-gateway"

  frontend:
    build:
      context: ./frontend
    ports:
      - "3000:80"
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 200M

  lmstudio_langchain:
    build:
      context: ./lmstudio_langchain
    ports:
      - "8085:8085"
    volumes:
      - ./lmstudio_langchain:/app
    environment:
      - BASE_URL=http://localhost:1234/v1  # Replace with your LMStudio API endpoint
      - MODEL_NAME=local-model  # Replace with your model name
      - OPENAI_API_KEY=not-needed  # LMStudio doesn't require an API key
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    extra_hosts:
      - "host.docker.internal:host-gateway"
