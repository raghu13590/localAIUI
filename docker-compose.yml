services:
  searxng:
    image: searxng/searxng
    ports:
      - "8080:8080"
    volumes:
      - ./config:/etc/searxng
    environment:
      - SEARXNG_BASE_URL=http://localhost:8080/
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 500M

  ollama_langchain:
    build:
      context: ./ollama_langchain
    ports:
      - "8081:8081"
    volumes:
      - ./ollama_langchain:/app
    environment:
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    extra_hosts:
      - "host.docker.internal:host-gateway"

  frontend:
    build:
      context: ./frontend
    ports:
      - "3000:80"
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 200M

  lmstudio_langchain:
    build:
      context: ./lmstudio_langchain
    ports:
      - "8082:8082"
    volumes:
      - ./lmstudio_langchain:/app
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    extra_hosts:
      - "host.docker.internal:host-gateway"
